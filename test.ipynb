{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu up\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch modules\n",
    "with ofc  numpy  and pandas\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torch.nn  import functional as F\n",
    "import torch.optim as  optim \n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "\n",
    "  print(\"gpu up\")\n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)\n",
    "\n",
    "import random\n",
    "SEED= 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "regex and the tokenizers\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.ar import Arabic\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "enNLP = English()\n",
    "arNLP = Arabic()\n",
    "\n",
    "enTokenizer = Tokenizer(enNLP.vocab)\n",
    "arTokenizer =  Tokenizer(arNLP.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/nour/translation project/translator/data/ara-eng.txt\",delimiter=\"\\t\",names=[\"eng\",\"ar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>مرحبًا.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>اركض!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Help!</td>\n",
       "      <td>النجدة!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jump!</td>\n",
       "      <td>اقفز!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>قف!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24633</th>\n",
       "      <td>rising voices promoting a more linguistically ...</td>\n",
       "      <td>شاركنا تحدي ابداع ميم بلغتك الام تعزيزا للتنوع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24634</th>\n",
       "      <td>following last year s successful campaign we i...</td>\n",
       "      <td>استكمالا لنجاح حملة العام السابق ندعوكم للمشار...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24635</th>\n",
       "      <td>during last year s challenge we also met langu...</td>\n",
       "      <td>تعرفنا خلال تحدي العام الماضي على ابطال لغويين...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24636</th>\n",
       "      <td>to take part just follow the simple steps outl...</td>\n",
       "      <td>للمشاركة في التحدي اتبع الخطوات الموضحة على ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24637</th>\n",
       "      <td>you will also find links to some free web base...</td>\n",
       "      <td>ستجد ايضا روابط لمجموعة من منصات ابداع الميم ا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24638 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "0                                                    Hi.   \n",
       "1                                                   Run!   \n",
       "2                                                  Help!   \n",
       "3                                                  Jump!   \n",
       "4                                                  Stop!   \n",
       "...                                                  ...   \n",
       "24633  rising voices promoting a more linguistically ...   \n",
       "24634  following last year s successful campaign we i...   \n",
       "24635  during last year s challenge we also met langu...   \n",
       "24636  to take part just follow the simple steps outl...   \n",
       "24637  you will also find links to some free web base...   \n",
       "\n",
       "                                                      ar  \n",
       "0                                                مرحبًا.  \n",
       "1                                                  اركض!  \n",
       "2                                                النجدة!  \n",
       "3                                                  اقفز!  \n",
       "4                                                    قف!  \n",
       "...                                                  ...  \n",
       "24633  شاركنا تحدي ابداع ميم بلغتك الام تعزيزا للتنوع...  \n",
       "24634  استكمالا لنجاح حملة العام السابق ندعوكم للمشار...  \n",
       "24635  تعرفنا خلال تحدي العام الماضي على ابطال لغويين...  \n",
       "24636  للمشاركة في التحدي اتبع الخطوات الموضحة على ال...  \n",
       "24637  ستجد ايضا روابط لمجموعة من منصات ابداع الميم ا...  \n",
       "\n",
       "[24638 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "defining the tokenizers for arabic and english  \n",
    "\n",
    "creating the fields for the dataset from torchtext \n",
    "that class is the simple way I could find for turning a df into a torch dataset\n",
    "\n",
    "نهها and ببدأ are just arbitrary words for init and end of sentence tokens  \n",
    "for some reason when I choose an arabic word for the unknown token  the vocab doesn't replace words that are not in the vocab  \n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from torchtext.data import Field, Dataset, Example\n",
    "import pandas as pd\n",
    "\n",
    "# Define tokenizers\n",
    "def myTokenizerEN(x):\n",
    "    return [word.text for word in enTokenizer(re.sub(r\"\\s+\\s+\", \" \", re.sub(r\"[\\.\\'\\`\\\"\\r+\\n+]\", \" \", x.lower())).strip())]\n",
    "\n",
    "def myTokenizerAR(x):\n",
    "    return [word.text for word in arTokenizer(re.sub(r\"\\s+\\s+\", \" \", re.sub(r\"[\\.\\'\\`\\\"\\r+\\n+]\", \" \", x.lower())).strip())]\n",
    "\n",
    "# Define fields\n",
    "SRC = Field(tokenize=myTokenizerEN, batch_first=False, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "TARGET = Field(tokenize=myTokenizerAR, batch_first=False, init_token=\"قف\", eos_token=\"مرحبًا\")\n",
    "\n",
    "# Define DataFrameDataset class\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, src_field, target_field, is_test=False, **kwargs):\n",
    "        fields = [('eng', src_field), ('ar', target_field)]\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            eng = row.eng \n",
    "            ar = row.ar\n",
    "            examples.append(Example.fromlist([eng, ar], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "# Example DataFrame\n",
    "data_dict = {'eng': ['Hello', 'How are you?'], 'ar': ['مرحبا', 'كيف حالك؟']}\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Create torch dataset\n",
    "torchdataset = DataFrameDataset(df, SRC, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = []  # Define the split_data variable before using it\n",
    "if len(split_data) > 1:\n",
    "    valid_data = split_data[1]\n",
    "else:\n",
    "    valid_data = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data,min_freq=2)\n",
    "TARGET.build_vocab(train_data,min_freq=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Commonly used words\n",
    "print(TARGET.vocab.freqs.most_common(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we are using batches for validation and test set because of memory usage we can't pass the whole set at once\n",
    "\n",
    "try lowering the batch size if you are out of memory \n",
    "\"\"\"\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to point out one thing about the transformer what it could do is to enable \n",
    "training on the whole sequence at once but on really using it for translation it predicts the next word \n",
    "then it feeds the prediction into the sequence again until the model predict <eos> token (with a max length ofc)\n",
    "\n",
    "\"\"\"\n",
    "class TranslateTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        max_len,\n",
    "    ):\n",
    "        super(TranslateTransformer, self).__init__()\n",
    "        self.srcEmbeddings = nn.Embedding(src_vocab_size,embedding_size)\n",
    "        self.trgEmbeddings= nn.Embedding(trg_vocab_size,embedding_size)\n",
    "        self.srcPositionalEmbeddings= nn.Embedding(max_len,embedding_size)\n",
    "        self.trgPositionalEmbeddings= nn.Embedding(max_len,embedding_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0,1) == self.src_pad_idx\n",
    "\n",
    "        return src_mask.to(device)\n",
    "\n",
    "    def forward(self,x,trg):\n",
    "        src_seq_length = x.shape[0]\n",
    "        N = x.shape[1]\n",
    "        trg_seq_length = trg.shape[0]\n",
    "        #adding zeros is an easy way\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .reshape(src_seq_length,1)  + torch.zeros(src_seq_length,N) \n",
    "        ).to(device)\n",
    "        \n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .reshape(trg_seq_length,1)  + torch.zeros(trg_seq_length,N) \n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "        srcWords = self.dropout(self.srcEmbeddings(x.long()) +self.srcPositionalEmbeddings(src_positions.long()))\n",
    "        trgWords = self.dropout(self.trgEmbeddings(trg.long())+self.trgPositionalEmbeddings(trg_positions.long()))\n",
    "        \n",
    "        src_padding_mask = self.make_src_mask(x)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(device)\n",
    "        \n",
    "        \n",
    "        out = self.transformer(srcWords,trgWords, src_key_padding_mask=src_padding_mask,tgt_mask=trg_mask )\n",
    "        out= self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of english vocabulary: 4\n",
      "Size of arabic vocabulary: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nour/translation project/translator/env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "#No. of unique tokens in text\n",
    "src_vocab_size  = len(SRC.vocab)\n",
    "print(\"Size of english vocabulary:\",src_vocab_size)\n",
    "\n",
    "#No. of unique tokens in label\n",
    "trg_vocab_size =len(TARGET.vocab)\n",
    "print(\"Size of arabic vocabulary:\",trg_vocab_size)\n",
    "\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "max_len= 227\n",
    "embedding_size= 256\n",
    "src_pad_idx =SRC.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "\n",
    "model = TranslateTransformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    max_len\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_track = []\n",
    "loss_validation_track= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out,trg)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m stepLoss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/translation project/translator/env/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/translation project/translator/env/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/translation project/translator/env/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I'm using adagrad because it assigns bigger updates to less frequently updated weights so \n",
    "so thought it could be useful for words not used a lot \n",
    "\"\"\"\n",
    "\n",
    "optimizer = optim.Adagrad(model.parameters(),lr = 0.003)\n",
    "EPOCHS = 60\n",
    "\n",
    "\n",
    "pad_idx = SRC.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx) \n",
    "\n",
    "for i in range(50,EPOCHS):\n",
    "    stepLoss=[]\n",
    "    model.train() # the training mode for the model (applies dropout and batchnorms)\n",
    "    for batch  in train_iterator:\n",
    "        input_sentence = batch.eng.to(device)\n",
    "        trg = batch.ar.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_sentence,trg[:-1])\n",
    "        out = out.reshape(-1,trg_vocab_size)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = criterion(out,trg)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        stepLoss.append(loss.item())\n",
    "        \n",
    "\n",
    "    loss_track.append(np.mean(stepLoss))\n",
    "    print(\"train crossentropy at epoch {} loss: \".format(i),np.mean(stepLoss))\n",
    "    \n",
    "    stepValidLoss=[]\n",
    "    model.eval() # the evaluation mode for the model (doesn't apply dropout and batchNorm)\n",
    "    for batch  in valid_iterator:\n",
    "        input_sentence = batch.eng.to(device)\n",
    "        trg = batch.ar.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_sentence,trg[:-1])\n",
    "        out = out.reshape(-1,trg_vocab_size)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = criterion(out,trg)\n",
    "        \n",
    "        stepValidLoss.append(loss.item())\n",
    "  \n",
    "    loss_validation_track.append(np.mean(stepValidLoss))\n",
    "    print(\"validation crossentropy at epoch {} loss: \".format(i),np.mean(stepValidLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#the train loss after 50 epoch\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(60),loss_track,label=\"train loss\")\n",
    "plt.plot(range(60),loss_validation_track,label=\"valiadtion loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function takes some arguments and returns the translated arabic sentence \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def translate(model,sentence,srcField,targetField,srcTokenizer):\n",
    "    model.eval()\n",
    "    processed_sentence = srcField.process([srcTokenizer(sentence)]).to(device)\n",
    "    trg = [\"ببدأ\"]\n",
    "    for _ in range(60):\n",
    "        \n",
    "        trg_indecies = [targetField.vocab.stoi[word] for word in trg]\n",
    "        outputs = torch.Tensor(trg_indecies).unsqueeze(1).to(device)\n",
    "        outputs = model(processed_sentence,outputs)\n",
    "        \n",
    "        if targetField.vocab.itos[outputs.argmax(2)[-1:].item()] == \"<unk>\":\n",
    "            continue \n",
    "        trg.append(targetField.vocab.itos[outputs.argmax(2)[-1:].item()])\n",
    "        if targetField.vocab.itos[outputs.argmax(2)[-1:].item()] == \"نهها\":\n",
    "            break\n",
    "    return \" \".join([word for word in trg if word != \"<unk>\"][1:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model,\"I'm happy\" ,SRC,TARGET,myTokenizerEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
