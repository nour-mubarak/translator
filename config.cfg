[model]

sos_token=0
eos_token=1
max_length=10
teacher_forcing_ratio=0.5

[rnn]

layer_type=lstm
num_layers=2
hidden_size=256
decoder_dropout=0.1

[training]

epochs=60000
print_every=1000 
save_every=10000 
plot_every=100
lr=0.01
lr_step_divisor=3
lr_step_gamma=0.1