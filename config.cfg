[model]

sos_token=0
eos_token=1
max_length=10
teacher_forcing_ratio=0.5

[rnn]

layer_type=lstm
num_layers=2
hidden_size=256
decoder_dropout=0.2

[training]

epochs=200000
print_every=1000 
save_every=20000 
plot_every=100
lr=0.01
lr_step_divisor=1
lr_step_gamma=1